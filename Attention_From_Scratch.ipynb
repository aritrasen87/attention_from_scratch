{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aritrasen87/attention_from_scratch/blob/main/Attention_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGYkMLnJ2ZrV"
      },
      "source": [
        "**Self Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKI9mrRt2dEE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "L, d_k, d_v = 4, 8, 8          # L - length of the sentence\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI7vwGqu2mMV",
        "outputId": "b5ff003a-5130-4ac6-fbd9-012646b77fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 0.87757223  1.41105057  1.39977175  1.22420483  0.58558245  1.60610013\n",
            "   0.02729193  0.43193749]\n",
            " [ 0.58707526  0.01123825 -0.51827508 -0.15856551  0.69769709 -0.39694259\n",
            "  -0.39524898 -0.6928615 ]\n",
            " [-1.81231447  0.86402493  0.79163243 -0.36866392 -2.00432817 -1.12131051\n",
            "  -1.76826089  0.64017657]\n",
            " [-0.50982929 -1.24802906  1.23380436  0.25323889 -0.1446522   0.10588238\n",
            "  -0.47176492  1.60428027]]\n",
            "K\n",
            " [[ 1.04372588  0.83044211  2.09467399 -0.20020522  1.28666529 -0.16825947\n",
            "  -0.01522364  0.56527964]\n",
            " [-0.14427522 -0.84104147 -0.10414464  1.83733874  2.14565559  0.87855788\n",
            "   0.31199133  0.94681323]\n",
            " [ 0.79656527  1.02426189  0.71771687  1.00009548 -0.19819071 -1.41670711\n",
            "   0.35728425 -0.31602399]\n",
            " [ 1.21224347  1.5831578  -1.39003495 -0.85074681  0.28261228 -0.95817732\n",
            "  -0.1001057   0.55875529]]\n",
            "V\n",
            " [[-1.85183784 -1.65793419 -1.24690088 -1.36093298  0.71160803 -0.93550661\n",
            "  -0.17420779 -0.19620049]\n",
            " [-0.14054867 -1.27902799  0.19040732  0.66091605 -0.03280491  0.40301596\n",
            "  -0.43660164  1.75476216]\n",
            " [ 1.76586225  0.92368156 -0.27979701 -2.28019852 -1.51233818 -1.32340423\n",
            "   0.08190005  0.85361202]\n",
            " [-1.0567463  -0.80870142  0.40131415 -0.86361657 -0.44110424 -0.60403313\n",
            "   1.02464218  0.14676636]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUE20lpk2pxJ",
        "outputId": "49d968d5-0c71-4bbc-db8a-9e09d0591a2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.501671  ,  3.87512541,  1.85510829, -0.82429374],\n",
              "       [ 0.14705528,  0.03744071,  0.45041819,  1.81473546],\n",
              "       [-1.44344677, -6.45630206,  0.79255475, -0.57315409],\n",
              "       [ 1.67529451,  2.61440866, -1.34251646, -3.72304565]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "np.matmul(q, k.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P2qr0pc20M1",
        "outputId": "5793bac2-1177-44b6-ddc1-f55718eedc1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9640774696750958, 0.8444673954026192, 7.608977734337012)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Why we need sqrt(d_k) in denominator --- to reduce the variance after multiplication\n",
        "q.var(), k.var(), np.matmul(q, k.T).var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LqwDfNn3LqD",
        "outputId": "10276d42-6d71-4421-f140-ae0d99ff81e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9640774696750958, 0.8444673954026192, 0.9511222167921263)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()  # reduced variance after scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hSCvvmq3Tur",
        "outputId": "f74a4d4e-ccea-4ae4-edb1-86fd20c0fafd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.94513443,  1.37006373,  0.65587983, -0.29143185],\n",
              "       [ 0.05199189,  0.01323729,  0.15924688,  0.64160587],\n",
              "       [-0.5103355 , -2.28264748,  0.28021042, -0.20264057],\n",
              "       [ 0.59230605,  0.92433304, -0.47465125, -1.31629541]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df61FcYe4IjU"
      },
      "source": [
        "**Masking**\n",
        "*   This is to ensure words don't get context from words generated in the future.\n",
        "*   Not required in the encoders, but required int he decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGhd910X3efL",
        "outputId": "426f1d69-a240-4e79-c76e-02164771c2bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "mask = np.tril(np.ones( (L, L) ))\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QRkmyu_4dYY"
      },
      "outputs": [],
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-igEzDoZ4tAN",
        "outputId": "869928fa-9e4d-480e-dc0e-852e144680ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Os9RSeS4vUx",
        "outputId": "04b23b74-a57b-4e95-ffee-45c0098f92b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.94513443,        -inf,        -inf,        -inf],\n",
              "       [ 0.05199189,  0.01323729,        -inf,        -inf],\n",
              "       [-0.5103355 , -2.28264748,  0.28021042,        -inf],\n",
              "       [ 0.59230605,  0.92433304, -0.47465125, -1.31629541]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "scaled + mask # -inf will become zero after softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xkTiewy6YKq"
      },
      "source": [
        "Softmax applied to scaled + mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQDHmAKL6Fi-"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rox5BGiS6XRn"
      },
      "outputs": [],
      "source": [
        "attention = softmax(scaled + mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c91qMRB46gol",
        "outputId": "c3991ad2-80bc-404d-fc18-08618638cd59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.50968744, 0.49031256, 0.        , 0.        ],\n",
              "       [0.29633675, 0.05035936, 0.65330389, 0.        ],\n",
              "       [0.34648451, 0.48292681, 0.11920931, 0.05137937]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvbihLp6maI",
        "outputId": "323c830b-368d-46d5-92ee-167da561cc5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.85183784, -1.65793419, -1.24690088, -1.36093298,  0.71160803,\n",
              "        -0.93550661, -0.17420779, -0.19620049],\n",
              "       [-1.01277126, -1.47215172, -0.54217062, -0.36959501,  0.34661302,\n",
              "        -0.27921218, -0.30286279,  0.76038101],\n",
              "       [ 0.59779913,  0.0477269 , -0.54270624, -1.85967371, -0.77879283,\n",
              "        -1.1215145 , -0.02010553,  0.58789534],\n",
              "       [-0.5532956 , -1.12356456, -0.35281419, -0.46856108,  0.02777035,\n",
              "        -0.31830828, -0.20879823,  0.88874053]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "new_v = np.matmul(attention, v)\n",
        "new_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZFJXKHh66P-"
      },
      "source": [
        "Self Attention Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2cZeafW6y-P"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "  return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBHAY_rl8apg",
        "outputId": "fffb19f6-f11c-4232-e4a5-4b99348b3831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 0.87757223  1.41105057  1.39977175  1.22420483  0.58558245  1.60610013\n",
            "   0.02729193  0.43193749]\n",
            " [ 0.58707526  0.01123825 -0.51827508 -0.15856551  0.69769709 -0.39694259\n",
            "  -0.39524898 -0.6928615 ]\n",
            " [-1.81231447  0.86402493  0.79163243 -0.36866392 -2.00432817 -1.12131051\n",
            "  -1.76826089  0.64017657]\n",
            " [-0.50982929 -1.24802906  1.23380436  0.25323889 -0.1446522   0.10588238\n",
            "  -0.47176492  1.60428027]]\n",
            "K\n",
            " [[ 1.04372588  0.83044211  2.09467399 -0.20020522  1.28666529 -0.16825947\n",
            "  -0.01522364  0.56527964]\n",
            " [-0.14427522 -0.84104147 -0.10414464  1.83733874  2.14565559  0.87855788\n",
            "   0.31199133  0.94681323]\n",
            " [ 0.79656527  1.02426189  0.71771687  1.00009548 -0.19819071 -1.41670711\n",
            "   0.35728425 -0.31602399]\n",
            " [ 1.21224347  1.5831578  -1.39003495 -0.85074681  0.28261228 -0.95817732\n",
            "  -0.1001057   0.55875529]]\n",
            "V\n",
            " [[-1.85183784 -1.65793419 -1.24690088 -1.36093298  0.71160803 -0.93550661\n",
            "  -0.17420779 -0.19620049]\n",
            " [-0.14054867 -1.27902799  0.19040732  0.66091605 -0.03280491  0.40301596\n",
            "  -0.43660164  1.75476216]\n",
            " [ 1.76586225  0.92368156 -0.27979701 -2.28019852 -1.51233818 -1.32340423\n",
            "   0.08190005  0.85361202]\n",
            " [-1.0567463  -0.80870142  0.40131415 -0.86361657 -0.44110424 -0.60403313\n",
            "   1.02464218  0.14676636]]\n",
            "New V\n",
            " [[-1.85183784 -1.65793419 -1.24690088 -1.36093298  0.71160803 -0.93550661\n",
            "  -0.17420779 -0.19620049]\n",
            " [-1.01277126 -1.47215172 -0.54217062 -0.36959501  0.34661302 -0.27921218\n",
            "  -0.30286279  0.76038101]\n",
            " [ 0.59779913  0.0477269  -0.54270624 -1.85967371 -0.77879283 -1.1215145\n",
            "  -0.02010553  0.58789534]\n",
            " [-0.5532956  -1.12356456 -0.35281419 -0.46856108  0.02777035 -0.31830828\n",
            "  -0.20879823  0.88874053]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.50968744 0.49031256 0.         0.        ]\n",
            " [0.29633675 0.05035936 0.65330389 0.        ]\n",
            " [0.34648451 0.48292681 0.11920931 0.05137937]]\n"
          ]
        }
      ],
      "source": [
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4X5OTzS_dD0"
      },
      "source": [
        "**Multihead Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxa9CqlB8iPu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uztt-i_O-yb"
      },
      "outputs": [],
      "source": [
        "sequence_length = 4\n",
        "batch_size = 1\n",
        "input_dim = 512\n",
        "d_model = 512 # model output\n",
        "x = torch.randn( (batch_size, sequence_length, input_dim) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtkY1cSwPIPx",
        "outputId": "56fbad86-8250-4686-b55a-6e25b28bffc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2274, -0.2506,  2.4013,  ...,  0.8887,  0.7475,  1.1164],\n",
              "         [-1.0460, -0.9515,  1.1646,  ...,  0.3300, -0.6437, -0.1216],\n",
              "         [-0.2700,  1.1487,  0.3804,  ..., -1.5539, -0.8287,  0.2541],\n",
              "         [ 0.2532, -1.0821, -0.6582,  ...,  1.1065, -0.9620,  2.5151]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcz5MR_uPIHM",
        "outputId": "34aa5d8a-9a9b-497b-93c5-17f0fdcfed32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj7df5oVPH8l"
      },
      "outputs": [],
      "source": [
        "qkv_layer = nn.Linear(input_dim , 3 * d_model) # q, k ,v vectors concatenated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWvalH9IRlS8"
      },
      "outputs": [],
      "source": [
        "qkv = qkv_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5lrOSRsRpE5",
        "outputId": "1de00594-76c7-4a3c-c6ea-9f731e7ce48b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "qkv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4vYsuJpRrIi",
        "outputId": "0f088c53-01ac-481d-bb8e-2708a612e083"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "512*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Q_uR0GicRtYB",
        "outputId": "eca9eaab-7935-4164-aa5f-85474e195263"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'qkv distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
        "x_val = np.arange(-1, 1, 0.01) * 3\n",
        "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
        "plt.title('qkv distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRffNg1CR27S"
      },
      "outputs": [],
      "source": [
        "num_heads = 8\n",
        "head_dim = d_model // num_heads\n",
        "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2QOvokySGbq",
        "outputId": "94f6ae61-2b81-42e9-8186-14dbdb80b18a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "d_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKC5JAGjSJTi",
        "outputId": "03556aad-8c17-45b3-fa84-e459ceb9c4fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 8, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "qkv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A2XpfO5SWOQ",
        "outputId": "57677d6a-e70a-400e-91d6-4753e3018ad2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "head_dim    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWPkR6a0Sk90",
        "outputId": "1fd73262-922d-4217-cf2e-8a858d6141f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6qkCwqISyGp",
        "outputId": "2d781ba0-3aa2-4c32-ffb6-ae1cb8decf1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "q, k, v = qkv.chunk(3, dim=-1)\n",
        "q.shape, k.shape, v.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX05LEwwTCU4"
      },
      "source": [
        "Self Attention for multiple heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8c6kiEQS8k6",
        "outputId": "c2443445-a322-4a59-b834-1a4526389a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "d_k = q.size()[-1]\n",
        "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # transpose to match the seq length , so that o/p is seq_len * seq_len\n",
        "scaled.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBUOCG24VdWc",
        "outputId": "4ea4dc87-2c5f-4cfb-8f3e-f86a6b1697ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 4, 64]), torch.Size([1, 8, 4, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "k.shape , q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK9D9AHrTYA5",
        "outputId": "5da44e20-e566-461d-b2b2-51011be14a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-879c2705464e>:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
            "  k.T.shape\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 4, 8, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "k.T.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRMGeKq4VKIg",
        "outputId": "96a61f04-1228-4ee8-b4ab-1c9a4d7a60c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 64, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "k.transpose(-1, -2).shape     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYTDrIb8VYCo"
      },
      "outputs": [],
      "source": [
        "# mask creation\n",
        "mask = torch.full(scaled.size() , float('-inf'))\n",
        "mask = torch.triu(mask, diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njH_bC9vWB1j",
        "outputId": "fe2bab3e-104d-4ee8-97ba-9011bb4bde62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "mask[0][0] # first mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZxS5WJjWC44",
        "outputId": "e8d5b04a-ebd3-44e6-964d-88b88193f14d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6855,    -inf,    -inf,    -inf],\n",
              "        [ 0.1394, -0.1385,    -inf,    -inf],\n",
              "        [ 0.3950, -0.3658,  0.2897,    -inf],\n",
              "        [ 0.4977, -0.2434,  0.0819, -0.8292]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "(scaled + mask)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT-F4bqtWOQh"
      },
      "outputs": [],
      "source": [
        "scaled += mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKN81wDdWR5T"
      },
      "outputs": [],
      "source": [
        "attention = F.softmax(scaled, dim=-1) # row by row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxBN8UZ8WZk4",
        "outputId": "0a374658-7834-44c0-afad-32a0a89d0a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW7rR7BtWcQI",
        "outputId": "153831d9-094a-4799-a0a9-bb882434d499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5690, 0.4310, 0.0000, 0.0000],\n",
              "        [0.4224, 0.1974, 0.3802, 0.0000],\n",
              "        [0.4164, 0.1984, 0.2747, 0.1105]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "attention[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv0fEEY7WgVg",
        "outputId": "3e36b7db-13e4-4fc1-f4a9-3fd42ecd1aae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "values = torch.matmul(attention, v) # new_values = attention * value vector --- this is more context aware\n",
        "values.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxObnUztWtz_"
      },
      "source": [
        "Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mGiQwsEWjQQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KULbtrKFW0Fh"
      },
      "outputs": [],
      "source": [
        "values, attention = scaled_dot_product(q, k, v, mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMwHP9wbW29w",
        "outputId": "3523fdda-6629-4f7a-fd9e-f715d09957dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 4, 4]), torch.Size([1, 8, 4, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "attention.shape , values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-XXilHGW72x",
        "outputId": "b54a92c6-ca35-438c-b915-44781a4496c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n",
        "values.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOseyha2ZKkx"
      },
      "outputs": [],
      "source": [
        "linear_layer = nn.Linear(d_model, d_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = linear_layer(values)"
      ],
      "metadata": {
        "id": "e64CEeIaaUcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0eoNb6NaXnn",
        "outputId": "b41f1a08-7c0d-48ed-de18-627b33bd1bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgGVNe24aahP",
        "outputId": "1bc2f9a5-e35d-4c28-c7c6-fae0fec152b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0302,  0.3632,  0.5183,  ..., -0.0262,  0.4259, -0.0604],\n",
              "         [ 0.0390,  0.3322, -0.1811,  ...,  0.0118, -0.0961,  0.1335],\n",
              "         [ 0.3549,  0.3774, -0.1839,  ..., -0.0942, -0.2115, -0.0393],\n",
              "         [ 0.0883,  0.1465,  0.2897,  ..., -0.0872, -0.0476,  0.0149]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class : MultiHead Attention"
      ],
      "metadata": {
        "id": "uiLvWYhDaxHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out"
      ],
      "metadata": {
        "id": "E3S9fqHlacm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "batch_size = 30\n",
        "sequence_length = 5\n",
        "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
        "\n",
        "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
        "out = model.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5LAOXKxbCJW",
        "outputId": "dd9116f4-5632-4a63-d7d1-6f2926ffdb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.size(): torch.Size([30, 5, 1024])\n",
            "qkv.size(): torch.Size([30, 5, 1536])\n",
            "qkv.size(): torch.Size([30, 5, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 5, 192])\n",
            "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
            "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
            "values.size(): torch.Size([30, 5, 512])\n",
            "out.size(): torch.Size([30, 5, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**"
      ],
      "metadata": {
        "id": "97z_co5Mbq4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_sequence_length = 10\n",
        "d_model = 6"
      ],
      "metadata": {
        "id": "r1MfkDYtbE-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "even_i = torch.arange(0, d_model, 2).float()\n",
        "even_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS14pP7nzg7b",
        "outputId": "33ef3df4-ec59-4012-abf8-e13dfcc59c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even_denominator = torch.pow(10000, even_i/d_model)\n",
        "even_denominator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqVRPqlIzkeb",
        "outputId": "a99c9f2e-a0a6-469e-a11a-ab3d5bddd7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_i = torch.arange(1, d_model, 2).float()\n",
        "odd_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyOCGoSNzqQj",
        "outputId": "3a66b6a7-e22d-4a98-c6c0-95259022fb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 3., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even_denominator = torch.pow(10000, (odd_i - 1)/d_model)\n",
        "even_denominator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hISui6S0Pz-",
        "outputId": "9cc3ebd9-85d3-4583-c4e8-fac552b11e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "even_denominator and odd_denominator are the same! So we can just do one of these actions and call the resulting variable denominator"
      ],
      "metadata": {
        "id": "_HD4WFQk0ZU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "denominator = even_denominator"
      ],
      "metadata": {
        "id": "YL2_dbey0ep1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position = torch.arange(max_sequence_length, dtype=torch.float).reshape(max_sequence_length, 1)"
      ],
      "metadata": {
        "id": "C2yjPzHw0etO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6Cxq1IO04HL",
        "outputId": "0149153f-ee63-4ba4-b164-20b1286096d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [6.],\n",
              "        [7.],\n",
              "        [8.],\n",
              "        [9.]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even_PE = torch.sin(position / denominator)\n",
        "odd_PE = torch.cos(position / denominator)"
      ],
      "metadata": {
        "id": "Vq-8FvTQ06sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "even_PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0h2Ouay1BHL",
        "outputId": "fd10a060-3f1e-44c1-bbce-db319aa28083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000],\n",
              "        [ 0.8415,  0.0464,  0.0022],\n",
              "        [ 0.9093,  0.0927,  0.0043],\n",
              "        [ 0.1411,  0.1388,  0.0065],\n",
              "        [-0.7568,  0.1846,  0.0086],\n",
              "        [-0.9589,  0.2300,  0.0108],\n",
              "        [-0.2794,  0.2749,  0.0129],\n",
              "        [ 0.6570,  0.3192,  0.0151],\n",
              "        [ 0.9894,  0.3629,  0.0172],\n",
              "        [ 0.4121,  0.4057,  0.0194]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even_PE.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hcYb1fI1FfN",
        "outputId": "75fdb03e-6425-44c5-ffdf-4f1988d67828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUxbpwhG1J-k",
        "outputId": "d6530842-ccef-46c4-e7ba-c0e5af7acdc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0000,  1.0000,  1.0000],\n",
              "        [ 0.5403,  0.9989,  1.0000],\n",
              "        [-0.4161,  0.9957,  1.0000],\n",
              "        [-0.9900,  0.9903,  1.0000],\n",
              "        [-0.6536,  0.9828,  1.0000],\n",
              "        [ 0.2837,  0.9732,  0.9999],\n",
              "        [ 0.9602,  0.9615,  0.9999],\n",
              "        [ 0.7539,  0.9477,  0.9999],\n",
              "        [-0.1455,  0.9318,  0.9999],\n",
              "        [-0.9111,  0.9140,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_PE.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wc6bbum1wjn",
        "outputId": "a8574324-d82b-4ff7-9905-74df8d6e010d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "stacked.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-_cb-YF1xFH",
        "outputId": "e3797ad0-258c-47d8-dbf5-25ab7a6def16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN1R7uEZ10rW",
        "outputId": "ec247e39-dcd2-456e-a8bf-d61a15e310ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class\n",
        "Let's combine all the code above into a cute class"
      ],
      "metadata": {
        "id": "KFVLhKcN2FIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE"
      ],
      "metadata": {
        "id": "BuRTgiZN2Bzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(d_model=6, max_sequence_length=10)\n",
        "pe.forward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYEoZx2D2I_K",
        "outputId": "c0734fc4-8dde-4d4a-ec68-e7a2168ac6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Normalization**"
      ],
      "metadata": {
        "id": "YU1cb-er5qto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.Tensor([[[0.2, 0.1, 0.3], [0.5, 0.1, 0.1]]])\n",
        "B, S, E = inputs.size()\n",
        "inputs = inputs.reshape(S, B, E)\n",
        "inputs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaQUwDJf5qHX",
        "outputId": "4d2c43ea-c9d5-4519-dd7e-0eff5048fec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameter_shape = inputs.size()[-2:]\n",
        "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
        "beta =  nn.Parameter(torch.zeros(parameter_shape))"
      ],
      "metadata": {
        "id": "xvMfhHM52NKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma.size(), beta.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khuNATZu7TdQ",
        "outputId": "17930a65-3ced-4387-e89c-7ea31f13ee4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3]), torch.Size([1, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [-(i + 1) for i in range(len(parameter_shape))]"
      ],
      "metadata": {
        "id": "j1jilujw7WXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dims"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHStyDJ07YcQ",
        "outputId": "0d4386dd-dc9d-4973-9444-eb21542b6536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1, -2]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = inputs.mean(dim=dims, keepdim=True)\n",
        "mean.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osq1BNaG7aXx",
        "outputId": "cc3e558e-b42c-485b-b93c-734db0d717ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRIycvM-7gNb",
        "outputId": "6d7d8f30-2512-4b71-8dc5-695dd0d3dfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2000]],\n",
              "\n",
              "        [[0.2333]]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "epsilon = 1e-5\n",
        "std = (var + epsilon).sqrt()\n",
        "std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSh7hUC37dWw",
        "outputId": "83cd2867-fcdd-45d8-d609-4b0788f6de41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0817]],\n",
              "\n",
              "        [[0.1886]]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = (inputs - mean) / std\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MLB0Rln7hxL",
        "outputId": "1ec3f448-5fc3-403e-c89f-a871de306943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
              "\n",
              "        [[ 1.4140, -0.7070, -0.7070]]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = gamma * y + beta"
      ],
      "metadata": {
        "id": "xkwkVVoN7lCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVjXZxT67oh4",
        "outputId": "60443c16-8263-45e9-f590-e3d24da6b656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
              "\n",
              "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LayerNormalization():\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, input):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        print(f\"Mean \\n ({mean.size()}): \\n {mean}\")\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        print(f\"Standard Deviation \\n ({std.size()}): \\n {std}\")\n",
        "        y = (inputs - mean) / std\n",
        "        print(f\"y \\n ({y.size()}) = \\n {y}\")\n",
        "        out = self.gamma * y  + self.beta\n",
        "        print(f\"out \\n ({out.size()}) = \\n {out}\")\n",
        "        return out"
      ],
      "metadata": {
        "id": "rQKHMpCU7qm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "sentence_length = 5\n",
        "embedding_dim = 8 \n",
        "inputs = torch.randn(sentence_length, batch_size, embedding_dim)\n",
        "\n",
        "print(f\"input \\n ({inputs.size()}) = \\n {inputs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMQPEKu37tUo",
        "outputId": "ab38c75f-8ba4-4792-d03c-06656c2e0667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input \n",
            " (torch.Size([5, 3, 8])) = \n",
            " tensor([[[-0.9644,  0.6828,  1.2079, -2.2441,  0.2633, -0.0366,  1.5427,\n",
            "           0.0068],\n",
            "         [-0.7202,  0.2460, -0.1259, -1.2465, -1.2472, -1.0066, -1.2364,\n",
            "           0.1491],\n",
            "         [-0.0915,  0.8822,  0.9283,  0.3437,  0.3083,  1.1175, -0.3567,\n",
            "           1.3288]],\n",
            "\n",
            "        [[-0.1334,  1.3315,  0.6787,  1.2111,  1.6449, -0.5478,  1.9520,\n",
            "           1.0336],\n",
            "         [-0.6436, -0.7713, -0.8779,  1.7093, -0.5216,  0.3714,  0.3243,\n",
            "          -0.1690],\n",
            "         [ 0.4332,  0.3065, -0.1248,  0.1267,  1.4477,  0.7700, -0.0874,\n",
            "           2.3913]],\n",
            "\n",
            "        [[-1.0006, -0.2248,  0.0112, -1.9032,  0.0424,  0.4494, -0.6080,\n",
            "          -1.5518],\n",
            "         [-1.3478,  0.3302, -0.6276, -0.7306,  1.3240,  1.0568, -0.2664,\n",
            "           0.2023],\n",
            "         [-0.2866, -0.1112, -0.6452,  1.8281,  0.4271,  1.6089, -0.3534,\n",
            "           0.0358]],\n",
            "\n",
            "        [[ 1.2079,  0.7964,  0.3402,  1.2852, -0.8252,  0.5133, -2.4035,\n",
            "          -0.1207],\n",
            "         [-1.7538,  0.5456,  0.1504,  0.0788,  1.0953,  0.1595,  0.7328,\n",
            "           0.0710],\n",
            "         [-0.2523, -1.3980, -0.5776, -1.1312,  0.5939,  0.6680, -0.0831,\n",
            "           0.6399]],\n",
            "\n",
            "        [[-0.1162, -0.7682, -0.5426, -0.7225, -0.6161,  0.3047, -0.7682,\n",
            "          -0.9855],\n",
            "         [ 2.5779, -0.0482, -1.2963, -1.0377, -1.0756, -1.6180,  1.0028,\n",
            "           0.7479],\n",
            "         [ 1.2416, -1.8825,  0.1305,  2.1757, -2.4201,  0.6280,  2.3779,\n",
            "           0.9632]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blowing up the Encoder"
      ],
      "metadata": {
        "id": "d5NeDqbt8BBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    print(f\"scaled.size() : {scaled.size()}\")\n",
        "    if mask is not None:\n",
        "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
        "        # Broadcasting add. So just the last N dimensions need to match\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        print(f\"Mean ({mean.size()})\")\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        print(f\"Standard Deviation  ({std.size()})\")\n",
        "        y = (inputs - mean) / std\n",
        "        print(f\"y: {y.size()}\")\n",
        "        out = self.gamma * y  + self.beta\n",
        "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
        "        print(f\"out: {out.size()}\")\n",
        "        return out\n",
        "\n",
        "  \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        print(f\"x after first linear layer: {x.size()}\")\n",
        "        x = self.relu(x)\n",
        "        print(f\"x after activation: {x.size()}\")\n",
        "        x = self.dropout(x)\n",
        "        print(f\"x after dropout: {x.size()}\")\n",
        "        x = self.linear2(x)\n",
        "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual_x = x\n",
        "        print(\"------- ATTENTION 1 ------\")\n",
        "        x = self.attention(x, mask=None)\n",
        "        print(\"------- DROPOUT 1 ------\")\n",
        "        x = self.dropout1(x)\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        print(\"------- ATTENTION 2 ------\")\n",
        "        x = self.ffn(x)\n",
        "        print(\"------- DROPOUT 2 ------\")\n",
        "        x = self.dropout2(x)\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                     for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8CXvAeHh8FHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5\n",
        "\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
      ],
      "metadata": {
        "id": "ati2WfGoaUnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes positional encoding\n",
        "out = encoder(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xU2RqUjaVoF",
        "outputId": "82d6afc9-69bd-4c55-9c29-6469cf516b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsNjlygllEYSHnBctCvrTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}